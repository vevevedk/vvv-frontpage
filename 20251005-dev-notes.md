# Dev Notes - October 5, 2025

## üéØ **Primary Focus: Twinkl GSC-Powered Keyword Intelligence System**

**Client**: Twinkl (Educational Resources)  
**Goal**: Automated PPC keyword opportunity detection from Google Search Console data using AI classification  
**Timeline**: Production deployment by Monday (October 7, 2025)

## üìä **Yesterday's Achievements (October 4, 2025)**

### ‚úÖ **WooCommerce Sync Issues - COMPLETELY RESOLVED**
- **26 Paid Search orders** recovered and properly classified
- **41 missing orders** captured from Aug 30 - Sep 3 period
- **100% data coverage** matching manual Sheet10 classification
- **Robust sync process** with comprehensive validation tools
- **Complete documentation** and deployment scripts created

### ‚úÖ **Technical Infrastructure Improvements**
- Fixed pagination bugs in WooCommerce sync
- Enhanced field mapping for all database constraints
- Extended sync window from 7 to 30 days with 5-minute buffer
- Added comprehensive validation and monitoring tools
- Created automated deployment scripts

## üñ•Ô∏è **Infrastructure Update: DigitalOcean Droplet Upgrade**

**Server**: DigitalOcean droplet `209.38.98.109`

- **Previous**: Basic, Premium AMD, 1 vCPU, 2 GB RAM, 50 GB SSD, $14/mo
- **Upgraded**: Basic, Regular, 2 vCPUs, 2 GB RAM, 60 GB SSD, 3 TB transfer, $18/mo
- **Delta**: +1 vCPU, +10 GB SSD, +1 TB transfer, +$4/mo

**Applications hosted**:
- `veveve.dk` (high priority, project next week)
- `invest-veveve.dk` (trading analytics)
- `smagalagellerup.dk`

**Expected impact**:
- Faster responses via 2x CPU, fewer timeouts/hangs
- Relief on disk pressure (+20% storage)
- Increased bandwidth (+50% transfer)

**Immediate actions (next 24h)**:
- [ ] Monitor performance and stability across all three apps
- [ ] Document observed changes (CPU, disk, response times)
- [ ] Smoke-test key flows for all apps

**This week (before project)**:
- [ ] Establish performance baselines and simple monitoring/alerts
- [ ] Verify backups and document recovery steps

**Post-project migration outline**:
- Migrate `smagalagellerup.dk` to tiny droplet ($6‚Äì12/mo)
- Move `veveve.dk` to dedicated droplet ($14‚Äì24/mo)
- Evaluate `invest-veveve.dk` for dedicated droplet based on DB load

## üöÄ **Today's Development Plan**

### **Phase 1: GSC System Foundation (Morning)**

#### **1.1 Create GSC Django App**
```bash
# Create new Django app for GSC functionality
cd backend
python manage.py startapp gsc
```

**Files to create:**
- `backend/gsc/apps.py` - App configuration
- `backend/gsc/models.py` - GSC data models with proper indexes
- `backend/gsc/views.py` - API endpoints for GSC operations
- `backend/gsc/serializers.py` - Data serialization
- `backend/gsc/tasks.py` - Celery tasks for data processing
- `backend/gsc/management/commands/` - Django management commands

#### **1.2 Database Schema Implementation**
**Core Models Required:**
```python
class GSCSearchTerm(models.Model):
    # Primary GSC data storage with comprehensive indexes
    account = models.ForeignKey(Account, on_delete=models.CASCADE)
    search_term = models.CharField(max_length=500)
    date = models.DateField()
    impressions = models.PositiveIntegerField()
    clicks = models.PositiveIntegerField()
    ctr = models.FloatField()
    avg_position = models.FloatField()
    country = models.CharField(max_length=2, default='US')
    device = models.CharField(max_length=20, default='desktop')
    
    class Meta:
        indexes = [
            models.Index(fields=['account', 'search_term']),
            models.Index(fields=['account', 'date']),
            models.Index(fields=['account', 'date', 'search_term']),  # Composite
            models.Index(fields=['date', 'impressions']),  # Date-range + volume
            models.Index(fields=['avg_position']),  # Position filtering
        ]

class SearchTermClassification(models.Model):
    # AI classification results with audit trail
    account = models.ForeignKey(Account, on_delete=models.CASCADE)
    search_term = models.CharField(max_length=500)
    ppc_priority = models.CharField(max_length=20)  # high/medium/low
    intent = models.CharField(max_length=50)  # commercial/informational/navigational
    estimated_cpc = models.DecimalField(max_digits=10, decimal_places=2, null=True)
    classifier_type = models.CharField(max_length=20)  # manual/ai
    confidence_score = models.FloatField(null=True)
    review_status = models.CharField(max_length=20, default='pending')
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        unique_together = ['account', 'search_term']
        indexes = [
            models.Index(fields=['account', 'ppc_priority', 'intent']),
            models.Index(fields=['classifier_type']),
            models.Index(fields=['review_status']),
        ]

class ClassificationCost(models.Model):
    # Cost tracking for AI API usage
    account = models.ForeignKey(Account, on_delete=models.CASCADE)
    search_term = models.CharField(max_length=500)
    tokens_used = models.PositiveIntegerField()
    cost_usd = models.DecimalField(max_digits=10, decimal_places=4)
    model_used = models.CharField(max_length=50)
    created_at = models.DateTimeField(auto_now_add=True)
```

#### **1.3 GSC API Integration**
**Key Components:**
- **GSCClient** with proper pagination (100k+ terms support)
- **Error handling** for rate limits (429) and server errors (500+)
- **Authentication** using existing Google OAuth setup
- **Data validation** and transformation pipeline

```python
class GSCClient:
    def __init__(self, credentials_path, property_url):
        self.credentials_path = credentials_path
        self.property_url = property_url
        self.service = self._build_service()
    
    def fetch_data(self, start_date, end_date, dimensions=['query', 'page', 'country', 'device']):
        """Fetch ALL GSC data with proper pagination - handles 100k+ terms"""
        all_rows = []
        start_row = 0
        row_limit = 25000
        
        while True:
            try:
                response = self.service.searchanalytics().query(
                    siteUrl=self.property_url,
                    body={
                        'startDate': start_date,
                        'endDate': end_date,
                        'dimensions': dimensions,
                        'rowLimit': row_limit,
                        'startRow': start_row
                    }
                ).execute()
                
                rows = response.get('rows', [])
                if not rows:
                    break
                    
                all_rows.extend(rows)
                
                if len(rows) < row_limit:
                    break  # Last page
                    
                start_row += row_limit
                
            except HttpError as e:
                if e.resp.status == 429:  # Rate limit
                    time.sleep(60)
                    continue
                elif e.resp.status >= 500:  # Server error
                    logger.error(f"GSC server error: {e}")
                    time.sleep(30)
                    continue
                else:
                    raise
            except Exception as e:
                logger.error(f"Unexpected error: {e}")
                break
        
        return all_rows
```

### **Phase 2: AI Classification System (Afternoon)**

#### **2.1 AI Classification Engine**
```python
class AIClassifier:
    MAX_DAILY_COST = 100.00  # $100/day limit per account
    
    def classify_term_gpt4(self, search_term, impressions, clicks, ctr, avg_position, countries):
        """Classify search term using GPT-4 with cost tracking"""
        self._check_daily_budget()
        
        prompt = f"""
        Analyze this search term for PPC opportunity:
        Term: "{search_term}"
        Impressions: {impressions:,}
        Clicks: {clicks:,}
        CTR: {ctr:.2f}%
        Avg Position: {avg_position:.1f}
        Countries: {', '.join(countries)}
        
        Classify as:
        1. PPC Priority: high/medium/low
        2. Intent: commercial/informational/navigational
        3. Estimated CPC: $X.XX (if commercial)
        
        Respond in JSON format.
        """
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.1
            )
            
            cost = self._track_cost(search_term, response.usage.total_tokens)
            return self._parse_classification(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"AI classification failed for '{search_term}': {e}")
            raise
```

#### **2.2 Batch Processing System**
```python
@shared_task(bind=True, max_retries=3)
def classify_batch_terms(self, account_id, search_terms_data):
    """Process batch of search terms for classification"""
    try:
        account = Account.objects.get(id=account_id)
        classifier = AIClassifier(account)
        
        classifications = []
        for term_data in search_terms_data:
            try:
                classification = classifier.classify_term_gpt4(**term_data)
                classifications.append(classification)
                
                # Update progress
                self.update_state(
                    state='PROGRESS',
                    meta={'current': len(classifications), 'total': len(search_terms_data)}
                )
                
            except Exception as e:
                logger.error(f"Failed to classify {term_data['search_term']}: {e}")
                continue
        
        # Bulk create classifications
        SearchTermClassification.objects.bulk_create(classifications)
        
        return {'success': True, 'classified': len(classifications)}
        
    except Exception as e:
        raise self.retry(exc=e, countdown=60 * (2 ** self.request.retries))
```

### **Phase 3: Frontend Integration (Evening)**

#### **3.1 GSC Dashboard Component**
**Location**: `components/gsc/GSCDashboard.tsx`

**Key Features:**
- **Search term overview** with filtering and sorting
- **Classification queue** with batch processing
- **Opportunity detection** with caching
- **Cost monitoring** with daily limits
- **Progress tracking** for large datasets

#### **3.2 Classification Interface**
**Location**: `components/gsc/ClassificationTool.tsx`

**Features:**
- **Batch saving queue** with optimistic updates
- **Real-time progress** indicators
- **Manual override** capabilities
- **Review workflow** for AI classifications
- **Export functionality** for classified terms

```typescript
const [saveQueue, setSaveQueue] = useState<ClassificationData[]>([]);

const handleSave = async () => {
    setSaveQueue(prev => [...prev, {
        search_term: currentTerm.search_term,
        ...classification
    }]);
    setCurrentIndex(currentIndex + 1);
};

useEffect(() => {
    const interval = setInterval(async () => {
        if (saveQueue.length > 0) {
            await axios.post('/api/gsc/classify_batch/', {
                account_id: accountId,
                classifications: saveQueue
            });
            setSaveQueue([]);
        }
    }, 5000);
    return () => clearInterval(interval);
}, [saveQueue]);
```

## üîß **Technical Improvements Identified**

### **1. Codebase Structure Enhancements**
- **Missing GSC app** - Need to create dedicated Django app
- **Incomplete database indexes** - Several models lack proper indexing
- **Limited error handling** - Need comprehensive error recovery
- **Missing caching layer** - Performance optimization needed
- **Incomplete test coverage** - Need unit and integration tests

### **2. Infrastructure Improvements**
- **Rate limiting** - Current limits too restrictive (100/hour ‚Üí 300/hour)
- **Monitoring gaps** - Need health checks and alerting
- **Deployment automation** - Enhance existing scripts
- **Documentation** - API documentation and setup guides

### **3. Performance Optimizations**
- **Database queries** - N+1 query issues in several views
- **Frontend rendering** - Large datasets need virtualization
- **API response times** - Need caching and pagination
- **Memory usage** - Large data processing needs optimization

## üìã **Implementation Checklist**

### **üî¥ CRITICAL (Must Complete Today)**
- [ ] Create GSC Django app with proper structure
- [ ] Implement GSC models with complete indexes
- [ ] Setup GSC API client with error handling
- [ ] Create AI classification engine with cost tracking
- [ ] Implement batch processing for large datasets
- [ ] Add caching to OpportunityDetector
- [ ] Create basic frontend components
- [ ] Setup Celery beat schedule for monitoring

### **üü° HIGH PRIORITY (Weekend)**
- [ ] Complete frontend dashboard implementation
- [ ] Add comprehensive error handling
- [ ] Implement monitoring and alerting
- [ ] Create management commands for data import
- [ ] Add unit tests for core functionality
- [ ] Setup production deployment scripts

### **üü¢ MEDIUM PRIORITY (Next Week)**
- [ ] Performance optimization and caching
- [ ] Advanced analytics and reporting
- [ ] Export and integration features
- [ ] User experience improvements
- [ ] Documentation and training materials

## üéØ **Success Metrics**

### **Technical Metrics**
- **GSC pagination**: Handle 100k+ search terms
- **AI cost control**: Stay under $100/day per account
- **Classification rate**: 300 classifications/hour
- **Error recovery**: 100% of failed tasks retry successfully
- **Response time**: <2 seconds for dashboard loads

### **Business Metrics**
- **Data accuracy**: 95%+ classification accuracy
- **Coverage**: 100% of high-impression terms classified
- **Efficiency**: 80% reduction in manual classification time
- **ROI**: Positive return within 30 days

## üöÄ **Deployment Strategy**

### **Today (Saturday)**
1. **Morning**: Create GSC app and models
2. **Afternoon**: Implement AI classification system
3. **Evening**: Basic frontend integration

### **Tomorrow (Sunday)**
1. **Morning**: Complete frontend dashboard
2. **Afternoon**: Testing and validation
3. **Evening**: Production deployment preparation

### **Monday**
1. **Morning**: Production deployment
2. **Afternoon**: Monitoring and optimization
3. **Evening**: Client handover and training

## üí° **Key Technical Decisions**

### **1. Database Design**
- **Separate GSC app** for clean separation of concerns
- **Comprehensive indexing** for performance with large datasets
- **Audit trails** for all classifications and costs
- **Flexible configuration** for different account requirements

### **2. AI Integration**
- **GPT-4 Turbo** for highest accuracy
- **Cost tracking** with daily limits per account
- **Batch processing** for efficiency
- **Manual override** capabilities for edge cases

### **3. Frontend Architecture**
- **React components** with TypeScript
- **Batch processing** with optimistic updates
- **Real-time progress** indicators
- **Responsive design** for all devices

## üìö **Resources and Documentation**

### **Existing Codebase Integration**
- **Account system**: Use existing `Account` and `AccountConfiguration` models
- **Authentication**: Leverage existing JWT authentication
- **API structure**: Follow existing REST API patterns
- **Frontend**: Integrate with existing dashboard layout

### **External APIs**
- **Google Search Console API**: For data ingestion
- **OpenAI GPT-4 API**: For AI classification
- **Google OAuth**: For authentication (existing setup)

## üéâ **Expected Outcomes**

By end of today:
- ‚úÖ **Complete GSC system foundation** with database models and API integration
- ‚úÖ **Working AI classification** with cost tracking and batch processing
- ‚úÖ **Basic frontend interface** for data viewing and classification
- ‚úÖ **Production-ready architecture** for Monday deployment

By Monday:
- ‚úÖ **Full production system** deployed and operational
- ‚úÖ **Client onboarding** complete with training materials
- ‚úÖ **Monitoring and alerting** active for system health
- ‚úÖ **Documentation** complete for maintenance and scaling

**Status: üöÄ READY TO BEGIN - All planning complete, implementation starting**

---

*Next update: End of day progress report*

